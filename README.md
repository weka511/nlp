# NLP

My experiments with Natural Language Processing

File | Description
-----------------|---------------------------------------------------------------
bt.py|Explore variability of Bradley-Terry
plot.py|Plot learning curves for a single corpus
rnn.py|[Sean Robertsons's NLP demo: Classifying Names](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
rnn2.py|[Sean Robertsons's NLP demo: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
rnn3.py|[Sean Robertsons's NLP demo: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
tokenizer.py|Prepare text for processing
word2vec.py |This program has been written to test my understanding of [word2vec](https://arxiv.org/abs/1301.3781/Word2Vec). The code was originally based on [Mateusz Bednarski's article--Implementing word2vec in PyTorch](https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb)
nlp.wpr|Wing IDE Project file
64317-0.txt|The Great Gatsby
chapter1.txt|First chapter of The Great Gatsby
unigram_freq.csv|Most frequent 333,333 word in English after [Rachel Tatman](https://www.kaggle.com/rtatman/english-word-frequenc)

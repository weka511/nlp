# NLP

My experiments with Natural Language Processing

File | Description
-----------------|---------------------------------------------------------------
bt.py|Explore variability of Bradley-Terry
plot.py|Plot learning curves for a single corpus
rnn.py|[Sean Robertsons's NLP demo: Classifying Names](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
rnn2.py|[Sean Robertsons's NLP demo: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
seq2seq.py|[Sean Robertsons's NLP demo: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
skipgram.py|This program has been written to test my understanding of [word2vec](https://arxiv.org/abs/1301.3781/Word2Vec). It includes code for training the weights explicitly, using a hand-coded stochastic gradient optimizer and loss function.
template.py|Template for new code with command line inerface
template-test.py|Template for new code using python unittest
tfidf.py|Implementation of td-idf algorithm
tfidf-harness.py|Test harness for td-idf algorithm
tokenizer.py|Prepare text for processing
word2vec.py |This program has been written to test my understanding of [word2vec](https://arxiv.org/abs/1301.3781/Word2Vec). The code was originally based on [Mateusz Bednarski's article--Implementing word2vec in PyTorch](https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb)
word2vec2.py|Test harness for skipgram.py. It builds examples, trains weights, and some test code.
nlp.wpr|Wing IDE Project file
64317-0.txt|The Great Gatsby
gatsby*.txt|Chapters of The Great Gatsby used for training
unigram_freq.csv|Most frequent 333,333 word in English after [Rachel Tatman](https://www.kaggle.com/rtatman/english-word-frequenc)
